# Global: Distributed KV Storage with Consensus

A production-ready distributed key-value storage system built on Raft consensus for the Ergo Framework. Provides strong consistency guarantees, distributed locking with fencing tokens, and automatic failure recovery.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Global KV Storage Cluster                        â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚     Node 1       â”‚  â”‚     Node 2       â”‚  â”‚     Node 3       â”‚  â”‚
â”‚  â”‚   (Leader)       â”‚  â”‚   (Follower)     â”‚  â”‚   (Follower)     â”‚  â”‚
â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ Global Actor â”‚ â”‚  â”‚ â”‚ Global Actor â”‚ â”‚  â”‚ â”‚ Global Actor â”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - KV Ops    â”‚ â”‚  â”‚ â”‚  - KV Ops    â”‚ â”‚  â”‚ â”‚  - KV Ops    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Locks     â”‚ â”‚  â”‚ â”‚  - Locks     â”‚ â”‚  â”‚ â”‚  - Locks     â”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Repl Mgr  â”‚ â”‚  â”‚ â”‚  - Repl Mgr  â”‚ â”‚  â”‚ â”‚  - Repl Mgr  â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚        â”‚         â”‚  â”‚        â”‚         â”‚  â”‚        â”‚         â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚Raft Consensusâ”‚â—„â”œâ”€â”€â”¼â”€â”¤Raft Consensusâ”‚â—„â”œâ”€â”€â”¼â”€â”¤Raft Consensusâ”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Election  â”‚â”€â”¼â”€â–ºâ”‚ â”‚  - Election  â”‚â”€â”¼â”€â–ºâ”‚ â”‚  - Election  â”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Heartbeat â”‚ â”‚  â”‚ â”‚  - Heartbeat â”‚ â”‚  â”‚ â”‚  - Heartbeat â”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Log Repl  â”‚ â”‚  â”‚ â”‚  - Log Repl  â”‚ â”‚  â”‚ â”‚  - Log Repl  â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚        â”‚         â”‚  â”‚        â”‚         â”‚  â”‚        â”‚         â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚Storage Engineâ”‚ â”‚  â”‚ â”‚Storage Engineâ”‚ â”‚  â”‚ â”‚Storage Engineâ”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Data      â”‚ â”‚  â”‚ â”‚  - Data      â”‚ â”‚  â”‚ â”‚  - Data      â”‚ â”‚  â”‚
â”‚  â”‚ â”‚  - Snapshots â”‚ â”‚  â”‚ â”‚  - Snapshots â”‚ â”‚  â”‚ â”‚  - Snapshots â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Clients   â”‚
    â”‚  - Apps     â”‚
    â”‚  - Services â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

### 1. Global Actor

The main actor that combines Raft consensus with KV storage operations and distributed locking.

```go
type GlobalActor struct {
    leader.Actor  // Embed Raft leader election

    // Storage
    store         StorageEngine
    log           []LogEntry
    commitIndex   uint64
    lastApplied   uint64
    snapshots     SnapshotManager

    // Distributed Locks
    locks         map[string]*Lock
    lockWaiters   map[string][]*LockWaiter
    tokenCounter  uint64  // Monotonic fencing token

    // Replication State (per-peer)
    nextIndex     map[gen.PID]uint64
    matchIndex    map[gen.PID]uint64

    // Async Operation Tracking
    pendingWrites map[uint64]*PendingWrite
    pendingReads  map[gen.Ref]*PendingRead

    // Configuration
    config        GlobalConfig
}
```

### 2. Raft Consensus Layer

Leverages `~/devel/ergo.services/actor/leader` for:
- **Leader Election**: Automatic leader selection with term-based voting
- **Heartbeat Protocol**: Leader liveness detection
- **Peer Discovery**: Automatic cluster membership management
- **Failure Detection**: Process monitoring and automatic failover
- **Split-Brain Prevention**: Quorum-based decision making

### 3. Storage Engine

Pluggable storage backend supporting multiple implementations:

```go
type StorageEngine interface {
    // Basic Operations
    Get(key string) ([]byte, bool, error)
    Put(key string, value []byte) error
    Delete(key string) error

    // Atomic Operations
    CAS(key string, old, new []byte) error

    // Batch Operations
    BatchPut(entries map[string][]byte) error
    BatchDelete(keys []string) error

    // Range Queries
    Range(start, end string, limit int) (Iterator, error)
    PrefixScan(prefix string, limit int) (Iterator, error)

    // Snapshots
    Snapshot() ([]byte, uint64, error)  // returns snapshot + lastIndex
    Restore(snapshot []byte, lastIndex uint64) error

    // Maintenance
    Compact() error
    Size() (int64, error)
    Close() error
}

type Iterator interface {
    Next() bool
    Key() string
    Value() []byte
    Error() error
    Close() error
}
```

**Implementations:**

- **MemoryStorage**: Fast in-memory map (volatile)
- **BadgerStorage**: Persistent LSM-tree storage (recommended)
- **BoltStorage**: B+tree embedded database
- **HybridStorage**: Write-ahead log + memory cache with persistence

### 4. Replication Protocol

#### Write Path (Linearizable)

```
Client â”€â”€Call(PUT)â”€â”€> Follower â”€â”€Forwardâ”€â”€> Leader
                                               â”‚
                                               â”œâ”€ Append to Log
                                               â”œâ”€ Broadcast AppendEntries
                                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                 â”‚
        â–¼                                                 â–¼
    Follower 1                                        Follower 2
        â”‚                                                 â”‚
        â”œâ”€ Append to Log                                 â”œâ”€ Append to Log
        â”œâ”€ Send AppendEntriesReply                       â”œâ”€ Send AppendEntriesReply
        â”‚                                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                           Leader
                               â”‚
                               â”œâ”€ Quorum Reached (3/3)
                               â”œâ”€ Commit Entry (commitIndex++)
                               â”œâ”€ Apply to Storage
                               â”‚
                               â””â”€ SendResponse(Client, Success)
```

**Key Features:**
- Async forwarding from followers to leader
- Direct response from leader to client (no double-hop)
- Pipelined replication for high throughput
- Per-operation fencing tokens

#### Read Path (Flexible Consistency)

**Linearizable Read (Default):**
```go
// Leader confirms leadership via heartbeat quorum before reading
if !IsLeader() {
    Forward to Leader
}
ReadBarrier()  // Wait for heartbeat quorum
return store.Get(key)
```

**Follower Read (Eventual Consistency):**
```go
// Fast local read, may be stale
return store.Get(key)
```

**ReadIndex Protocol (Linearizable Follower Read):**
```go
readIndex := CallLeader(RequestReadIndex)
WaitUntil(commitIndex >= readIndex)
return store.Get(key)
```

## Log Structure

```go
type LogEntry struct {
    Term      uint64      // Raft term when created
    Index     uint64      // Log position (1-indexed)
    Timestamp int64       // Unix nano
    Op        Operation   // The operation to apply
}

type Operation struct {
    Type  OpType
    Key   string
    Value []byte

    // For CAS operations
    Prev  []byte

    // For lock operations
    Owner gen.PID
    Token uint64
    TTL   int
}

type OpType int

const (
    OpPut OpType = iota
    OpDelete
    OpCAS
    OpBatchPut
    OpBatchDelete
    OpLockAcquire
    OpLockRelease
    OpNoOp  // For leader initialization
)
```

## Distributed Locking

### Design: Session-Based Locks with Fencing

**Features:**
- Automatic lock release on client failure (via process monitoring)
- Monotonic fencing tokens to prevent split-brain scenarios
- Optional TTL with automatic expiration
- Wait queue with timeout support
- Strong consistency via Raft replication

### Lock Structure

```go
type Lock struct {
    Key        string
    Owner      gen.PID      // Process holding the lock
    Token      uint64       // Fencing token (monotonically increasing)
    AcquiredAt time.Time
    TTL        int          // seconds, 0 = no expiration
    Term       uint64       // Raft term when acquired
}

type LockWaiter struct {
    Request LockRequest
    Caller  gen.PID
    Ref     gen.Ref
    Timeout time.Time
}
```

### Lock Operations

#### Acquire Lock

```go
type LockRequest struct {
    Key     string
    Owner   gen.PID      // Client process (for monitoring)
    TTL     int          // Optional time-to-live in seconds
    Timeout int          // Wait timeout in seconds (0 = no wait)
}

type LockResponse struct {
    Success bool
    Token   uint64       // Fencing token (use in protected operations)
    Error   string
}
```

**Flow:**
1. Client sends `LockRequest` to any node
2. Follower forwards to leader (async)
3. Leader checks lock availability:
   - **Available**: Generate fencing token, replicate via Raft
   - **Held**: Queue waiter if timeout > 0, else return failure
4. After quorum commit:
   - Apply lock to state machine
   - Monitor lock owner for failures
   - Respond to client with token
5. Client receives `LockResponse` with fencing token

#### Release Lock

```go
type UnlockRequest struct {
    Key   string
    Token uint64         // Must match to prevent unauthorized unlock
}
```

**Flow:**
1. Client sends `UnlockRequest`
2. Leader verifies token matches
3. Replicate release via Raft
4. After commit:
   - Remove lock from state
   - Demonitor owner
   - Grant lock to next waiter (if any)

#### Automatic Lock Release

**On Client Failure:**
```go
func (g *GlobalActor) HandleMessage(from gen.PID, message any) error {
    switch msg := message.(type) {
    case gen.MessageDownPID:
        // Client died - release all their locks
        for key, lock := range g.locks {
            if lock.Owner == msg.PID {
                g.releaseViaRaft(key, lock.Token)
            }
        }
    }
}
```

**On TTL Expiration:**
```go
// Periodic check (every second)
func (g *GlobalActor) checkLockTimeouts() {
    now := time.Now()
    for key, lock := range g.locks {
        if lock.TTL > 0 {
            elapsed := now.Sub(lock.AcquiredAt)
            if elapsed > time.Duration(lock.TTL)*time.Second {
                g.releaseViaRaft(key, lock.Token)
            }
        }
    }
}
```

### Fencing Tokens

Prevent split-brain and stale lock holders from corrupting state:

```go
type WriteWithToken struct {
    Data  []byte
    Token uint64  // Lock token from LockResponse
}

// Protected resource actor
func (r *ResourceActor) HandleMessage(from gen.PID, message any) error {
    switch msg := message.(type) {
    case WriteWithToken:
        // Reject operations with stale tokens
        if msg.Token < r.lastSeenToken {
            return errors.New("stale token - lock expired or revoked")
        }

        r.lastSeenToken = msg.Token
        return r.applyWrite(msg.Data)
    }
}
```

**Why Fencing Tokens Are Critical:**

Scenario without fencing:
1. Client A acquires lock with TTL=10s
2. Network partition: Client A isolated
3. TTL expires, Client B acquires lock
4. Both A and B think they hold the lock
5. Data corruption from concurrent writes

With fencing tokens:
1. Client A gets Token=100
2. Client B gets Token=101 (after A's TTL expires)
3. Resource rejects A's writes (Token=100 < 101)
4. Only B's writes succeed

## Message Protocol

### Client Messages

```go
// KV Operations
type RequestGet struct {
    Key string
}

type RequestPut struct {
    Key   string
    Value []byte
}

type RequestDelete struct {
    Key string
}

type RequestCAS struct {
    Key      string
    OldValue []byte  // nil = must not exist
    NewValue []byte  // nil = delete if matches
}

type RequestBatchPut struct {
    Entries map[string][]byte
}

type RequestRange struct {
    Start string
    End   string
    Limit int
}

// Lock Operations
type LockRequest struct {
    Key     string
    Owner   gen.PID
    TTL     int
    Timeout int
}

type UnlockRequest struct {
    Key   string
    Token uint64
}

// Responses
type Response struct {
    Success bool
    Value   []byte
    Error   string
}

type LockResponse struct {
    Success bool
    Token   uint64
    Error   string
}
```

### Internal Replication Messages

```go
type msgAppendEntries struct {
    Term         uint64
    LeaderID     gen.PID
    PrevLogIndex uint64
    PrevLogTerm  uint64
    Entries      []LogEntry
    LeaderCommit uint64
}

type msgAppendEntriesReply struct {
    Term       uint64
    Success    bool
    MatchIndex uint64
    Hint       uint64  // For log backtracking on failure
}

type msgInstallSnapshot struct {
    Term              uint64
    LeaderID          gen.PID
    LastIncludedIndex uint64
    LastIncludedTerm  uint64
    Data              []byte
}

type ForwardedRequest struct {
    From    gen.PID
    Ref     gen.Ref
    Request any
}
```

## API Design

### Client Library

```go
type GlobalClient struct {
    nodes   []gen.ProcessID
    process gen.Process  // For lock ownership tracking
}

// KV Operations
func (c *GlobalClient) Get(key string) ([]byte, error)
func (c *GlobalClient) Put(key string, value []byte) error
func (c *GlobalClient) Delete(key string) error
func (c *GlobalClient) CAS(key string, old, new []byte) error
func (c *GlobalClient) BatchPut(entries map[string][]byte) error
func (c *GlobalClient) Range(start, end string, limit int) (map[string][]byte, error)

// Lock Operations
func (c *GlobalClient) Lock(key string) (*LockHandle, error)
func (c *GlobalClient) LockWithTTL(key string, ttl int) (*LockHandle, error)
func (c *GlobalClient) TryLock(key string) (*LockHandle, error)
func (c *GlobalClient) LockWithTimeout(key string, ttl, timeout int) (*LockHandle, error)

// Convenience pattern
func (c *GlobalClient) WithLock(key string, fn func() error) error

type LockHandle struct {
    Key    string
    Token  uint64
    client *GlobalClient
}

func (h *LockHandle) Unlock() error
func (h *LockHandle) Token() uint64  // For fencing
```

### Usage Examples

#### Basic KV Operations

```go
client := global.NewClient([]gen.ProcessID{
    {Name: "global", Node: "node1"},
    {Name: "global", Node: "node2"},
    {Name: "global", Node: "node3"},
}, myProcess)

// Put
err := client.Put("user:123", []byte(`{"name":"Alice"}`))

// Get
value, err := client.Get("user:123")

// Delete
err := client.Delete("user:123")

// CAS (atomic update)
err := client.CAS("counter", []byte("10"), []byte("11"))

// Batch
err := client.BatchPut(map[string][]byte{
    "user:1": []byte(`{"name":"Alice"}`),
    "user:2": []byte(`{"name":"Bob"}`),
})

// Range scan
results, err := client.Range("user:", "user:~", 100)
```

#### Distributed Locking

```go
// Simple lock
lock, err := client.Lock("resource:payment:123")
if err != nil {
    return err
}
defer lock.Unlock()

// Critical section
processPayment()

// With TTL (auto-release after 30s)
lock, err := client.LockWithTTL("batch:job:1", 30)
if err != nil {
    return err
}
defer lock.Unlock()

runBatchJob()

// Try lock (non-blocking)
lock, err := client.TryLock("resource:cache:refresh")
if err != nil {
    // Someone else is already refreshing
    return nil
}
defer lock.Unlock()

refreshCache()

// Convenience pattern
err := client.WithLock("resource:db:migration", func() error {
    return runMigration()
})

// With fencing token (protect against split-brain)
lock, err := client.Lock("ledger:update")
if err != nil {
    return err
}
defer lock.Unlock()

// Use token in protected writes
err = ledgerActor.Write(WriteWithToken{
    Token: lock.Token(),
    Data:  updateData,
})
```

#### Actor Integration

```go
type WorkerActor struct {
    act.Actor
    globalClient *global.GlobalClient
}

func (w *WorkerActor) Init(args ...any) error {
    w.globalClient = global.NewClient(
        args[0].([]gen.ProcessID),
        w.Process,
    )
    return nil
}

func (w *WorkerActor) HandleMessage(from gen.PID, message any) error {
    switch msg := message.(type) {
    case ProcessTaskMsg:
        // Distributed lock ensures only one worker processes this task
        lock, err := w.globalClient.LockWithTimeout(
            "task:"+msg.ID,
            30,  // TTL: 30 seconds
            5,   // Wait up to 5 seconds
        )
        if err != nil {
            return err
        }
        defer lock.Unlock()

        // Process task exclusively
        return w.processTask(msg)
    }
}
```

## Implementation Phases

### Phase 1: Foundation (Week 1)

**Goal**: Basic Raft-replicated KV store with linearizable writes

**Tasks:**
- [ ] Create `GlobalActor` embedding `leader.Actor`
- [ ] Implement in-memory `StorageEngine`
- [ ] Define `LogEntry` and `Operation` structures
- [ ] Implement basic message handlers (Put/Get/Delete)
- [ ] Add async request forwarding from follower to leader
- [ ] Implement log replication (`AppendEntries` RPC)
- [ ] Add commit and apply logic

**Deliverables:**
- Working 3-node cluster with leader election
- Linearizable Put/Get/Delete operations
- Automatic failover on leader crash

### Phase 2: Replication Optimization (Week 2)

**Goal**: Efficient replication with pipelining and batching

**Tasks:**
- [ ] Add per-peer replication state (`nextIndex`, `matchIndex`)
- [ ] Implement pipelined `AppendEntries` (multiple in-flight)
- [ ] Add batch commit (commit multiple entries at once)
- [ ] Implement log backtracking on follower inconsistency
- [ ] Add replication metrics (lag, throughput)
- [ ] Implement `PendingWrite` tracking for async responses

**Deliverables:**
- High-throughput replication (10k+ ops/sec)
- Automatic recovery from network partitions
- Detailed replication metrics

### Phase 3: Distributed Locks (Week 3)

**Goal**: Session-based distributed locking with fencing

**Tasks:**
- [ ] Add lock state management (`locks`, `lockWaiters`)
- [ ] Implement `OpLockAcquire` and `OpLockRelease`
- [ ] Add monotonic fencing token generation
- [ ] Implement lock owner monitoring (`MessageDownPID`)
- [ ] Add TTL-based lock expiration
- [ ] Implement wait queue with timeouts
- [ ] Add lock metrics (held count, wait time)

**Deliverables:**
- Fully functional distributed locks
- Automatic release on client failure
- Fencing token support

### Phase 4: Persistence (Week 4)

**Goal**: Durable storage with crash recovery

**Tasks:**
- [ ] Implement `BadgerStorage` backend
- [ ] Add write-ahead log (WAL) for durability
- [ ] Implement snapshot creation and restoration
- [ ] Add log compaction (truncate after snapshot)
- [ ] Implement snapshot transfer (`InstallSnapshot` RPC)
- [ ] Add crash recovery on startup
- [ ] Implement storage metrics (size, compaction stats)

**Deliverables:**
- Persistent storage with BadgerDB
- Fast recovery from crashes
- Automatic log compaction

### Phase 5: Advanced Features (Week 5)

**Goal**: Production-ready optimizations

**Tasks:**
- [ ] Implement ReadIndex protocol for follower reads
- [ ] Add CAS and batch operations
- [ ] Implement range queries and prefix scans
- [ ] Add compression for large values
- [ ] Implement lease-based reads (optional)
- [ ] Add client-side caching
- [ ] Implement operational commands (status, peers, stats)

**Deliverables:**
- Read scalability via follower reads
- Rich query API
- Production operational tools

### Phase 6: Testing & Hardening (Week 6)

**Goal**: Production reliability

**Tasks:**
- [ ] Chaos testing (random node failures)
- [ ] Network partition testing (split-brain scenarios)
- [ ] Performance benchmarks (ops/sec, latency percentiles)
- [ ] Load testing (sustained high throughput)
- [ ] Correctness testing (Jepsen-style)
- [ ] Documentation and examples
- [ ] Monitoring and alerting setup

**Deliverables:**
- Comprehensive test suite
- Performance benchmarks
- Production deployment guide

## Configuration

```go
type GlobalConfig struct {
    // Raft Configuration
    ClusterID          string
    Bootstrap          []gen.ProcessID
    ElectionTimeoutMin int  // milliseconds, default: 150
    ElectionTimeoutMax int  // milliseconds, default: 300
    HeartbeatInterval  int  // milliseconds, default: 50

    // Storage Configuration
    StorageType        StorageType  // Memory, Badger, Bolt
    DataDir            string       // Path for persistent storage
    SnapshotInterval   int          // Entries between snapshots, default: 10000
    SnapshotThreshold  int          // Min entries before snapshot, default: 5000

    // Replication Configuration
    MaxEntriesPerMsg   int          // Max entries in AppendEntries, default: 100
    MaxInflightMsgs    int          // Max in-flight replication msgs, default: 10

    // Lock Configuration
    DefaultLockTTL     int          // Default TTL in seconds, 0 = no TTL
    LockCheckInterval  int          // TTL check interval, default: 1000ms
    MaxLockWaiters     int          // Max queued waiters per lock, default: 100

    // Performance Tuning
    ReadConsistency    ReadConsistency  // Linearizable, Follower, ReadIndex
    CompressValues     bool             // Compress large values
    CompressThreshold  int              // Min bytes to compress, default: 1024

    // Operational
    MetricsEnabled     bool
    LogLevel           gen.LogLevel
}

type StorageType int
const (
    StorageMemory StorageType = iota
    StorageBadger
    StorageBolt
)

type ReadConsistency int
const (
    ReadLinearizable ReadConsistency = iota
    ReadFollower
    ReadIndex
)
```

## Deployment

### Single-Node Development

```go
node, err := ergo.StartNode("node1", gen.NodeOptions{
    Network: gen.NetworkOptions{
        Mode: gen.NetworkModeEnabled,
        Cookie: "dev-secret",
        Acceptors: []gen.AcceptorOptions{{Port: 15001}},
    },
})

// Start Global application
globalApp := &global.Application{
    Config: global.GlobalConfig{
        ClusterID:   "dev-cluster",
        Bootstrap:   []gen.ProcessID{},  // Single node
        StorageType: global.StorageMemory,
    },
}

node.ApplicationStart(globalApp)
```

### Production 3-Node Cluster

**Node 1:**
```go
node1, err := ergo.StartNode("global1@host1", gen.NodeOptions{
    Network: gen.NetworkOptions{
        Mode: gen.NetworkModeEnabled,
        Cookie: "prod-secret-change-me",
        Acceptors: []gen.AcceptorOptions{{Port: 15001}},
    },
})

globalApp := &global.Application{
    Config: global.GlobalConfig{
        ClusterID: "prod-cluster",
        Bootstrap: []gen.ProcessID{
            {Name: "global", Node: "global1@host1"},
            {Name: "global", Node: "global2@host2"},
            {Name: "global", Node: "global3@host3"},
        },
        StorageType:      global.StorageBadger,
        DataDir:          "/var/lib/global/data",
        SnapshotInterval: 10000,
        ReadConsistency:  global.ReadLinearizable,
        MetricsEnabled:   true,
    },
}

node1.ApplicationStart(globalApp)
```

**Nodes 2 & 3**: Same configuration with respective node names

### Docker Compose Example

```yaml
version: '3.8'

services:
  global1:
    image: ergo-global:latest
    hostname: global1
    environment:
      NODE_NAME: global1@global1
      CLUSTER_ID: docker-cluster
      BOOTSTRAP: global1@global1,global2@global2,global3@global3
      DATA_DIR: /data
    volumes:
      - global1-data:/data
    ports:
      - "15001:15001"

  global2:
    image: ergo-global:latest
    hostname: global2
    environment:
      NODE_NAME: global2@global2
      CLUSTER_ID: docker-cluster
      BOOTSTRAP: global1@global1,global2@global2,global3@global3
      DATA_DIR: /data
    volumes:
      - global2-data:/data
    ports:
      - "15002:15001"

  global3:
    image: ergo-global:latest
    hostname: global3
    environment:
      NODE_NAME: global3@global3
      CLUSTER_ID: docker-cluster
      BOOTSTRAP: global1@global1,global2@global2,global3@global3
      DATA_DIR: /data
    volumes:
      - global3-data:/data
    ports:
      - "15003:15001"

volumes:
  global1-data:
  global2-data:
  global3-data:
```

## Consistency Guarantees

### Write Operations

**Linearizability**: All writes go through Raft consensus
- Leader appends to log
- Replicates to quorum (majority)
- Commits when majority acknowledges
- Applies to state machine
- Responds to client

**Properties:**
- Writes appear to execute atomically
- Once committed, all future reads see the write
- If write W1 completes before W2 starts, all nodes see W1 before W2

### Read Operations

#### Linearizable Reads (Default)

**Method**: Leader with read barrier
```go
func (g *GlobalActor) handleGet(req RequestGet) (Response, error) {
    if !g.IsLeader() {
        // Forward to leader
        return g.CallLeader(req)
    }

    // Wait for heartbeat quorum to confirm leadership
    g.readBarrier()

    return g.store.Get(req.Key)
}
```

**Guarantees:**
- Reads see all committed writes
- No stale reads even during leadership changes
- Consistent with linearizable writes

**Performance**: ~2x heartbeat latency

#### Follower Reads (Eventual Consistency)

**Method**: Read from local storage
```go
func (g *GlobalActor) handleGet(req RequestGet) (Response, error) {
    // Read directly from local storage
    return g.store.Get(req.Key)
}
```

**Guarantees:**
- May return stale data
- Bounded staleness (â‰¤ replication lag)
- High throughput, low latency

**Performance**: Local read latency (~Âµs)

#### ReadIndex Protocol (Linearizable Follower Reads)

**Method**: Confirm commit index with leader
```go
func (g *GlobalActor) handleGet(req RequestGet) (Response, error) {
    if g.IsLeader() {
        g.readBarrier()
    } else {
        readIndex, _ := g.CallLeader(RequestReadIndex{})
        g.waitForCommitIndex(readIndex)
    }

    return g.store.Get(req.Key)
}
```

**Guarantees:**
- Linearizable reads from followers
- Scales read throughput
- No stale reads

**Performance**: 1 leader RPC + wait for replication

### Lock Operations

**Strong Consistency**: All lock operations go through Raft
- Acquire/Release replicated to quorum
- Monotonic fencing tokens prevent split-brain
- Process monitoring ensures automatic cleanup

**Safety Properties:**
1. **Mutual Exclusion**: At most one client holds a lock at any time
2. **Deadlock-Free**: TTL ensures locks eventually release
3. **Fairness**: FIFO wait queue (optional)
4. **Fault-Tolerant**: Automatic release on client failure

## Performance Characteristics

### Expected Throughput

| Operation | Single Node | 3-Node Cluster | Notes |
|-----------|-------------|----------------|-------|
| Linearizable Write | N/A | 10k-50k ops/sec | Leader only, quorum latency |
| Linearizable Read | 100k ops/sec | 100k ops/sec | Leader only, read barrier |
| Follower Read | 200k ops/sec | 600k ops/sec | All nodes, no coordination |
| Lock Acquire | N/A | 5k-10k ops/sec | Quorum + monitor setup |
| Lock Release | N/A | 10k-20k ops/sec | Quorum only |

*Benchmarked on 3x 8-core, 32GB RAM, NVMe SSD, 1Gbps network*

### Latency

| Operation | p50 | p95 | p99 |
|-----------|-----|-----|-----|
| Write (LAN) | 2ms | 5ms | 10ms |
| Read (Leader) | 1ms | 3ms | 5ms |
| Read (Follower) | 50Âµs | 200Âµs | 500Âµs |
| Lock Acquire | 3ms | 8ms | 15ms |

### Storage

- **Memory overhead**: ~200 bytes per lock, ~100 bytes per pending operation
- **Disk space**: Data + log + snapshots
- **Log growth**: ~100 bytes per operation
- **Snapshot compression**: ~2:1 ratio (varies by data)

## Failure Scenarios

### Node Failures

| Scenario | Behavior | Recovery |
|----------|----------|----------|
| **Follower crash** | Leader continues, replication to N-1 | Auto-rejoin on restart, catch-up |
| **Leader crash** | New election (~300ms), brief unavailability | Automatic, new leader elected |
| **Minority failure** (1/3 nodes) | Full operation continues | Auto-recovery on restart |
| **Majority failure** (2/3 nodes) | Cluster unavailable, no writes/reads | Manual intervention, restore quorum |
| **All nodes crash** | Total outage | Restart all nodes, load from snapshots |

### Network Partitions

| Scenario | Behavior | Resolution |
|----------|----------|----------|
| **Leader isolated** | Leader steps down, minority partition unavailable | New leader in majority partition |
| **Minority isolated** (1/3 nodes) | Majority continues operation | Rejoin when partition heals |
| **Split-brain** (1+1 vs 1) | No partition has quorum, all unavailable | Wait for partition heal |

### Lock Failures

| Scenario | Behavior | Recovery |
|----------|----------|----------|
| **Lock holder crashes** | Automatic release via `MessageDownPID` | Next waiter granted lock |
| **TTL expires** | Automatic release, periodic check | Next waiter granted lock |
| **Leader changes during lock** | Lock state preserved in log | No interruption |
| **Client partition** | TTL expires, lock released | Client cannot renew, must re-acquire |

### Data Corruption

| Scenario | Behavior | Recovery |
|----------|----------|----------|
| **Corrupted log entry** | Checksum validation fails | Panic, manual recovery needed |
| **Corrupted snapshot** | Restore fails | Use previous snapshot or rebuild from log |
| **Disk full** | Writes fail, node becomes read-only | Free space, compact, resume |

## Operational Commands

```go
// Cluster status
type CmdStatus struct{}
type StatusResponse struct {
    NodeID      gen.PID
    IsLeader    bool
    Leader      gen.PID
    Term        uint64
    CommitIndex uint64
    LastApplied uint64
    Peers       []gen.PID
    LockCount   int
    StorageSize int64
}

// Peer management
type CmdAddPeer struct { Peer gen.ProcessID }
type CmdRemovePeer struct { Peer gen.PID }

// Snapshots
type CmdSnapshot struct{}  // Force snapshot creation
type CmdCompact struct{}   // Force log compaction

// Locks
type CmdListLocks struct{}
type LocksResponse struct {
    Locks []LockInfo
}

type LockInfo struct {
    Key        string
    Owner      gen.PID
    Token      uint64
    HeldFor    time.Duration
    TTL        int
    Waiters    int
}

// Metrics
type CmdMetrics struct{}
type MetricsResponse struct {
    WritesTotal       uint64
    ReadsTotal        uint64
    LocksTotal        uint64
    BytesStored       int64
    ReplicationLagMs  int64
    CommitLatencyMs   float64
}
```

## Monitoring & Observability

### Metrics (Prometheus format)

```
# Cluster
global_is_leader{node="global1@host1"} 1
global_term{node="global1@host1"} 42
global_peers_count{node="global1@host1"} 3

# Replication
global_commit_index{node="global1@host1"} 1000000
global_last_applied{node="global1@host1"} 1000000
global_replication_lag_ms{node="global1@host1",peer="global2@host2"} 2

# Operations
global_writes_total{node="global1@host1"} 1000000
global_reads_total{node="global1@host1"} 5000000
global_locks_acquired_total{node="global1@host1"} 10000
global_locks_released_total{node="global1@host1"} 9950

# Performance
global_commit_latency_ms{node="global1@host1",quantile="0.5"} 2
global_commit_latency_ms{node="global1@host1",quantile="0.95"} 5
global_commit_latency_ms{node="global1@host1",quantile="0.99"} 10

# Storage
global_storage_size_bytes{node="global1@host1"} 1073741824
global_log_entries{node="global1@host1"} 50000
global_snapshots_total{node="global1@host1"} 5

# Locks
global_locks_held{node="global1@host1"} 50
global_lock_waiters{node="global1@host1"} 5
```

### Health Checks

```go
// HTTP endpoint
GET /health
{
  "status": "ok",
  "is_leader": true,
  "has_quorum": true,
  "commit_lag": 0,
  "storage_ok": true
}
```

## Security Considerations

### Authentication

- Ergo network cookie authentication
- TLS support for inter-node communication
- Client authentication via process ownership

### Authorization

- Process-based ACLs (future)
- Lock ownership verification via fencing tokens
- Audit logging (future)

### Data Protection

- At-rest encryption (storage engine dependent)
- In-transit encryption (TLS)
- Snapshot encryption (future)

## Comparison with Alternatives

| Feature | Global | etcd | Consul | Zookeeper |
|---------|--------|------|--------|-----------|
| **Consensus** | Raft | Raft | Raft | ZAB (Paxos-like) |
| **Language** | Go | Go | Go | Java |
| **Actor Model** | âœ… Native | âŒ | âŒ | âŒ |
| **Distributed Locks** | âœ… Fencing | âœ… Sessions | âœ… Sessions | âœ… Ephemeral |
| **Watches** | ðŸ”„ Future | âœ… | âœ… | âœ… |
| **Transactions** | ðŸ”„ Future | âœ… | âœ… | âœ… |
| **TTL** | âœ… | âœ… | âœ… | âœ… (sessions) |
| **Range Queries** | âœ… | âœ… | âœ… | âŒ |
| **Follower Reads** | âœ… ReadIndex | âœ… | âœ… Stale | âŒ |

**Advantages:**
- Native integration with Ergo actor model
- Process-based lock ownership with automatic cleanup
- Async message-passing architecture
- No external dependencies beyond Ergo

**Use Cases:**
- Coordination for Ergo distributed applications
- Leader election for worker pools
- Distributed configuration
- Service discovery metadata
- Rate limiting counters
- Distributed semaphores

## Future Enhancements

### Short-term (3-6 months)
- [ ] Watch API for key change notifications
- [ ] Multi-key transactions with ACID guarantees
- [ ] Distributed semaphores (counted locks)
- [ ] Lease-based reads for even lower latency
- [ ] gRPC API for non-Ergo clients

### Long-term (6-12 months)
- [ ] Multi-Raft for horizontal sharding
- [ ] Cross-datacenter replication
- [ ] Conflict-free replicated data types (CRDTs)
- [ ] Time-series optimizations
- [ ] SQL-like query language
- [ ] Built-in backup/restore tools

## References

### Papers
- [In Search of an Understandable Consensus Algorithm (Raft)](https://raft.github.io/raft.pdf)
- [Implementing Linearizability at Large Scale](https://www.cockroachlabs.com/blog/living-without-atomic-clocks/)
- [How to Build a Highly Available System Using Consensus](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/How-to-Build-a-Highly-Available-System-Using-Consensus.pdf)

### Related Projects
- [Ergo Framework](https://github.com/ergo-services/ergo)
- [Raft Leader Actor](~/devel/ergo.services/actor/leader)
- [etcd](https://etcd.io/)
- [Consul](https://www.consul.io/)
- [Hashicorp Raft](https://github.com/hashicorp/raft)

## Contributing

See `CONTRIBUTING.md` for development setup, testing guidelines, and contribution process.

## License

Apache 2.0 - see `LICENSE` file for details.

---

**Status**: Design phase - implementation in progress

**Maintainer**: Ergo Framework Team

**Contact**: https://github.com/ergo-services/ergo
